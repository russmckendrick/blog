---
title: "Running LLMs locally with Ollama"
author: "Russ McKendrick"
date: 2024-03-28T13:01:41+01:00
description: "Playing with local LLMs in my freetime."
draft: true
showToc: true
cover:
    image: "cover.png"
    relative: true
tags:
  - "ai"
  - "code"
---

As I am coming to the end of writing the second edition of Lean Ansible (more news on that coming soon), I thought now would be a great time to have a look at what interesting has been happening in the last three or four months not that I have a little more free time.

One of the things I have been keeping an eye on is the state of Large Language Models (or LLM for short) , especially since the introduction of open-source models such as [Llama from Meta](https://llama.meta.com) and [Mistral 7B](https://mistral.ai/news/announcing-mistral-7b/) which you can run locally.

Luckily for me, the fact I have been busy writing has meant enough time has passed for deployment methods to be much more straightforward and streamlined. The first tool I will look at in this post is Ollama; while it has been available since July last year, it has come on leaps and bounds since November 2023.

# Ollama

## Installing on macOS



# Open WebUI

## Running on macOS

